{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks import EarlyStopping, LearningRateMonitor\n",
    "from torch.utils.data import Dataset,DataLoader,TensorDataset\n",
    "from pytorch_forecasting import TimeSeriesDataSet\n",
    "from pytorch_forecasting.models import BaseModel\n",
    "from pytorch_forecasting.data import GroupNormalizer\n",
    "from pytorch_forecasting import Baseline\n",
    "import matplotlib.pyplot as plt\n",
    "from pytorch_lightning import Trainer\n",
    "from typing import Dict\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "#os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>views</th>\n",
       "      <th>month</th>\n",
       "      <th>log_views</th>\n",
       "      <th>time_idx</th>\n",
       "      <th>dayofweek</th>\n",
       "      <th>year</th>\n",
       "      <th>avg_by_song</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(Everything_I_Do)_I_Do_It_for_You</td>\n",
       "      <td>2015-07-01</td>\n",
       "      <td>639</td>\n",
       "      <td>7</td>\n",
       "      <td>6.459904</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2015</td>\n",
       "      <td>682.548387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(Everything_I_Do)_I_Do_It_for_You</td>\n",
       "      <td>2015-07-02</td>\n",
       "      <td>647</td>\n",
       "      <td>7</td>\n",
       "      <td>6.472346</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2015</td>\n",
       "      <td>682.548387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(Everything_I_Do)_I_Do_It_for_You</td>\n",
       "      <td>2015-07-03</td>\n",
       "      <td>549</td>\n",
       "      <td>7</td>\n",
       "      <td>6.308098</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2015</td>\n",
       "      <td>682.548387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(Everything_I_Do)_I_Do_It_for_You</td>\n",
       "      <td>2015-07-04</td>\n",
       "      <td>638</td>\n",
       "      <td>7</td>\n",
       "      <td>6.458338</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>2015</td>\n",
       "      <td>682.548387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(Everything_I_Do)_I_Do_It_for_You</td>\n",
       "      <td>2015-07-05</td>\n",
       "      <td>774</td>\n",
       "      <td>7</td>\n",
       "      <td>6.651572</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>2015</td>\n",
       "      <td>682.548387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150686</th>\n",
       "      <td>You're_the_One_That_I_Want</td>\n",
       "      <td>2021-12-28</td>\n",
       "      <td>217</td>\n",
       "      <td>12</td>\n",
       "      <td>5.379897</td>\n",
       "      <td>2372</td>\n",
       "      <td>1</td>\n",
       "      <td>2021</td>\n",
       "      <td>191.064516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150687</th>\n",
       "      <td>You're_the_One_That_I_Want</td>\n",
       "      <td>2021-12-29</td>\n",
       "      <td>181</td>\n",
       "      <td>12</td>\n",
       "      <td>5.198497</td>\n",
       "      <td>2373</td>\n",
       "      <td>2</td>\n",
       "      <td>2021</td>\n",
       "      <td>191.064516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150688</th>\n",
       "      <td>You're_the_One_That_I_Want</td>\n",
       "      <td>2021-12-30</td>\n",
       "      <td>175</td>\n",
       "      <td>12</td>\n",
       "      <td>5.164786</td>\n",
       "      <td>2374</td>\n",
       "      <td>3</td>\n",
       "      <td>2021</td>\n",
       "      <td>191.064516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150689</th>\n",
       "      <td>You're_the_One_That_I_Want</td>\n",
       "      <td>2021-12-31</td>\n",
       "      <td>279</td>\n",
       "      <td>12</td>\n",
       "      <td>5.631212</td>\n",
       "      <td>2375</td>\n",
       "      <td>4</td>\n",
       "      <td>2021</td>\n",
       "      <td>191.064516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150690</th>\n",
       "      <td>You're_the_One_That_I_Want</td>\n",
       "      <td>2022-01-01</td>\n",
       "      <td>221</td>\n",
       "      <td>1</td>\n",
       "      <td>5.398163</td>\n",
       "      <td>2376</td>\n",
       "      <td>5</td>\n",
       "      <td>2022</td>\n",
       "      <td>221.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>121227 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  article  timestamp  views month  log_views  \\\n",
       "0       (Everything_I_Do)_I_Do_It_for_You 2015-07-01    639     7   6.459904   \n",
       "1       (Everything_I_Do)_I_Do_It_for_You 2015-07-02    647     7   6.472346   \n",
       "2       (Everything_I_Do)_I_Do_It_for_You 2015-07-03    549     7   6.308098   \n",
       "3       (Everything_I_Do)_I_Do_It_for_You 2015-07-04    638     7   6.458338   \n",
       "4       (Everything_I_Do)_I_Do_It_for_You 2015-07-05    774     7   6.651572   \n",
       "...                                   ...        ...    ...   ...        ...   \n",
       "150686         You're_the_One_That_I_Want 2021-12-28    217    12   5.379897   \n",
       "150687         You're_the_One_That_I_Want 2021-12-29    181    12   5.198497   \n",
       "150688         You're_the_One_That_I_Want 2021-12-30    175    12   5.164786   \n",
       "150689         You're_the_One_That_I_Want 2021-12-31    279    12   5.631212   \n",
       "150690         You're_the_One_That_I_Want 2022-01-01    221     1   5.398163   \n",
       "\n",
       "        time_idx dayofweek  year  avg_by_song  \n",
       "0              0         2  2015   682.548387  \n",
       "1              1         3  2015   682.548387  \n",
       "2              2         4  2015   682.548387  \n",
       "3              3         5  2015   682.548387  \n",
       "4              4         6  2015   682.548387  \n",
       "...          ...       ...   ...          ...  \n",
       "150686      2372         1  2021   191.064516  \n",
       "150687      2373         2  2021   191.064516  \n",
       "150688      2374         3  2021   191.064516  \n",
       "150689      2375         4  2021   191.064516  \n",
       "150690      2376         5  2022   221.000000  \n",
       "\n",
       "[121227 rows x 9 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"data/All_music.csv\")\n",
    "data.sort_values(by=[\"timestamp\"],inplace=True) # for my sanity\n",
    "data[\"timestamp\"] = pd.to_datetime(data[\"timestamp\"]) # needed to create time_index\n",
    "\n",
    "#group by month\n",
    "# data[\"time_idx\"] = data[\"timestamp\"].dt.year * 12 + data[\"timestamp\"].dt.month\n",
    "# data[\"time_idx\"] -= data[\"time_idx\"].min()\n",
    "\n",
    "data[\"month\"] = data[\"timestamp\"].dt.month.astype(str).astype(\"category\")\n",
    "data[\"log_views\"] = np.log(data[\"views\"] + 1e-8)\n",
    "\n",
    "\n",
    "data[\"time_idx\"] = data.index.astype(int)\n",
    "#data[\"timestamp\"] = data[\"timestamp\"].astype(str).astype('category')\n",
    "# (data.groupby(\"project\")[\"time_idx\"].diff() == 1).value_counts()\n",
    "\n",
    "\n",
    "# # add time index\n",
    "# data[\"time_idx\"] = data[\"date\"].dt.year * 12 + data[\"date\"].dt.month\n",
    "# data[\"time_idx\"] -= data[\"time_idx\"].min()\n",
    "data.drop([\"project\",\"granularity\",\"access\",\"agent\"],inplace=True,axis=1)\n",
    "data[\"article\"] = data[\"article\"].astype(str).astype(\"category\")\n",
    "\n",
    "# Delete wrong data\n",
    "data = data[data[\"article\"] != \"Paper_Doll\"]\n",
    "data = data[data[\"article\"] != \"Y.M.C.A\"]\n",
    "data = data[data[\"article\"] != \"Bei_Mir_Bist_Du_Schön\"]\n",
    "\n",
    "#new Features\n",
    "data[\"dayofweek\"] = data[\"timestamp\"].dt.dayofweek.astype(str).astype(\"category\")\n",
    "data[\"log_views\"] = np.log(data[\"views\"] + 1e-8)\n",
    "data[\"month\"] = data[\"timestamp\"].dt.month.astype(str).astype(\"category\")\n",
    "data[\"year\"] = data[\"timestamp\"].dt.year.astype(str).astype(\"category\")\n",
    "data[\"avg_by_song\"] = data.groupby([\"month\",\"year\",\"article\"],observed=True)[\"views\"].transform(\"mean\")\n",
    "\n",
    "\n",
    "data = data[data['article'].map(data['article'].value_counts()) == 2377]\n",
    "data.drop(\"Unnamed: 0\",inplace=True,axis=1)\n",
    "data.sort_values(by=[\"article\",\"timestamp\"],inplace=True)\n",
    "#data[\"time_idx\"] = data[\"timestamp\"].dt.year * 12 + data[\"timestamp\"].dt.month + 365 * data[\"timestamp\"].dt.day\n",
    "data[\"time_idx\"] = [i for i in range(2377)] * len(data[\"article\"].unique())\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mohammad Sakhnini\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:268: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Mohammad Sakhnini\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:268: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Mohammad Sakhnini\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch\\nn\\modules\\rnn.py:62: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "C:\\Users\\Mohammad Sakhnini\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:447: LightningDeprecationWarning: Setting `Trainer(gpus=1)` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=1)` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..\n"
     ]
    }
   ],
   "source": [
    "from torch.nn.utils import rnn\n",
    "\n",
    "from pytorch_forecasting.models.base_model import AutoRegressiveBaseModel\n",
    "from pytorch_forecasting.models.nn import LSTM\n",
    "\n",
    "\n",
    "class LSTMModel(AutoRegressiveBaseModel):\n",
    "    def __init__(\n",
    "        self,\n",
    "        target: str,\n",
    "        target_lags: Dict[str, Dict[str, int]],\n",
    "        n_layers: int,\n",
    "        hidden_size: int,\n",
    "        dropout: float = 0.1,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        # arguments target and target_lags are required for autoregressive models\n",
    "        # even though target_lags cannot be used without covariates\n",
    "        # saves arguments in signature to `.hparams` attribute, mandatory call - do not skip this\n",
    "        self.save_hyperparameters()\n",
    "        # pass additional arguments to BaseModel.__init__, mandatory call - do not skip this\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        # use version of LSTM that can handle zero-length sequences\n",
    "        self.lstm = LSTM(\n",
    "            hidden_size=self.hparams.hidden_size,\n",
    "            input_size=7,\n",
    "            num_layers=self.hparams.n_layers,\n",
    "            dropout=self.hparams.dropout,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.output_layer = nn.Linear(self.hparams.hidden_size, 1)\n",
    "\n",
    "    def encode(self, x: Dict[str, torch.Tensor]):\n",
    "        # we need at least one encoding step as because the target needs to be lagged by one time step\n",
    "        # because we use the custom LSTM, we do not have to require encoder lengths of > 1\n",
    "        # but can handle lengths of >= 1\n",
    "        assert x[\"encoder_lengths\"].min() >= 1\n",
    "        input_vector = x[\"encoder_cont\"].clone()\n",
    "        # lag target by one\n",
    "        input_vector[..., self.target_positions] = torch.roll(\n",
    "            input_vector[..., self.target_positions], shifts=1, dims=1\n",
    "        )\n",
    "        input_vector = input_vector[:, 1:]  # first time step cannot be used because of lagging\n",
    "\n",
    "        # determine effective encoder_length length\n",
    "        effective_encoder_lengths = x[\"encoder_lengths\"] - 1\n",
    "        # run through LSTM network\n",
    "        _, hidden_state = self.lstm(\n",
    "            input_vector, lengths=effective_encoder_lengths, enforce_sorted=False  # passing the lengths directly\n",
    "        )  # second ouput is not needed (hidden state)\n",
    "        return hidden_state\n",
    "\n",
    "    def decode(self, x: Dict[str, torch.Tensor], hidden_state):\n",
    "        # again lag target by one\n",
    "        input_vector = x[\"decoder_cont\"].clone()\n",
    "        input_vector[..., self.target_positions] = torch.roll(\n",
    "            input_vector[..., self.target_positions], shifts=1, dims=1\n",
    "        )\n",
    "        # but this time fill in missing target from encoder_cont at the first time step instead of throwing it away\n",
    "        last_encoder_target = x[\"encoder_cont\"][\n",
    "            torch.arange(x[\"encoder_cont\"].size(0), device=x[\"encoder_cont\"].device),\n",
    "            x[\"encoder_lengths\"] - 1,\n",
    "            self.target_positions.unsqueeze(-1),\n",
    "        ].T\n",
    "        input_vector[:, 0, self.target_positions] = last_encoder_target\n",
    "\n",
    "        if self.training:  # training mode\n",
    "            lstm_output, _ = self.lstm(input_vector, hidden_state, lengths=x[\"decoder_lengths\"], enforce_sorted=False)\n",
    "\n",
    "            # transform into right shape\n",
    "            prediction = self.output_layer(lstm_output)\n",
    "            prediction = self.transform_output(prediction, target_scale=x[\"target_scale\"])\n",
    "\n",
    "            # predictions are not yet rescaled\n",
    "            return prediction\n",
    "\n",
    "        else:  # prediction mode\n",
    "            target_pos = self.target_positions\n",
    "\n",
    "            def decode_one(idx, lagged_targets, hidden_state):\n",
    "                x = input_vector[:, [idx]]\n",
    "                # overwrite at target positions\n",
    "                x[:, 0, target_pos] = lagged_targets[-1]  # take most recent target (i.e. lag=1)\n",
    "                lstm_output, hidden_state = self.lstm(x, hidden_state)\n",
    "                # transform into right shape\n",
    "                prediction = self.output_layer(lstm_output)[:, 0]  # take first timestep\n",
    "                return prediction, hidden_state\n",
    "\n",
    "            # make predictions which are fed into next step\n",
    "            output = self.decode_autoregressive(\n",
    "                decode_one,\n",
    "                first_target=input_vector[:, 0, target_pos],\n",
    "                first_hidden_state=hidden_state,\n",
    "                target_scale=x[\"target_scale\"],\n",
    "                n_decoder_steps=input_vector.size(1),\n",
    "            )\n",
    "\n",
    "            # predictions are already rescaled\n",
    "            return output\n",
    "\n",
    "    def forward(self, x: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:\n",
    "        hidden_state = self.encode(x)  # encode to hidden state\n",
    "        output = self.decode(x, hidden_state)  # decode leveraging hidden state\n",
    "\n",
    "        return self.to_network_output(prediction=output)\n",
    "\n",
    "\n",
    "max_encoder_length = 365   #\n",
    "max_prediction_length = 14  # predict last 14 days\n",
    "test_cutoff = data.time_idx.max() - max_prediction_length\n",
    "training_cutoff = test_cutoff - max_prediction_length\n",
    "\n",
    "HIDDEN_SIZE = 16\n",
    "HIDDEN_CONTINOUS_SIZE = HIDDEN_SIZE // 2\n",
    "#create test set.\n",
    "data_test = data[lambda x:  test_cutoff < x.time_idx]\n",
    "\n",
    "# create train+val part\n",
    "data_seen = data[lambda x:  x.time_idx <= test_cutoff]\n",
    "\n",
    "\n",
    "dataset = TimeSeriesDataSet(\n",
    "    data[lambda x: x.time_idx <= training_cutoff],\n",
    "    time_idx=\"time_idx\",\n",
    "    target=\"views\",\n",
    "    group_ids=[\"article\"],\n",
    "    min_encoder_length=max_encoder_length // 2,\n",
    "    max_encoder_length=max_encoder_length,\n",
    "    min_prediction_length=1,\n",
    "    max_prediction_length=max_prediction_length,\n",
    "    target_normalizer=GroupNormalizer(groups=[\"article\"]),\n",
    "    time_varying_known_categoricals=[\"dayofweek\", \"month\", \"year\"],\n",
    "    time_varying_known_reals=[\"log_views\"],\n",
    "    static_categoricals=[\"article\"],\n",
    "    time_varying_unknown_reals=[\"avg_by_song\", \"log_views\"],\n",
    "    #allow_missing_timesteps=True,\n",
    "    add_relative_time_idx=True,\n",
    "    add_target_scales=True,\n",
    "    add_encoder_length=True)\n",
    "\n",
    "model = LSTMModel.from_dataset(dataset, n_layers=1, hidden_size=4)\n",
    "dataloader = dataset.to_dataloader(batch_size=4,shuffle=False)\n",
    "\n",
    "early_stop_callback = EarlyStopping(monitor=\"val_loss\", min_delta=1e-4, patience=10, verbose=False, mode=\"min\")\n",
    "lr_logger = LearningRateMonitor()  # log the learning rate\n",
    "\n",
    "trainer = Trainer(\n",
    "    max_epochs=20,\n",
    "    gpus=1,\n",
    "    gradient_clip_val=0.1,\n",
    "    #limit_train_batches=30,  # coment in for training, running valiation every 30 batches\n",
    "    limit_train_batches=1.0, #if set to 1.0 gather all training data, default.\n",
    "    callbacks=[lr_logger, early_stop_callback],\n",
    ")\n",
    "path = \"model_lstm.pth\"\n",
    "\n",
    "if os.path.exists(path):\n",
    "    model.load_state_dict(torch.load(path))\n",
    "    model.eval()\n",
    "else:\n",
    "    trainer.fit(\n",
    "    model,\n",
    "    train_dataloaders=dataloader,\n",
    "    val_dataloaders=dataloader,\n",
    ")\n",
    "#trainer.fit(model, train_dataloaders=dataloader, val_dataloaders=dataloader)\n",
    "#torch.save(model.state_dict(),path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[526., 623., 556.,  ..., 564., 545., 576.],\n",
      "        [623., 556., 586.,  ..., 545., 576., 532.],\n",
      "        [556., 586., 712.,  ..., 576., 532., 796.],\n",
      "        ...,\n",
      "        [586., 793., 638.,  ..., 508., 457., 644.],\n",
      "        [793., 638., 620.,  ..., 457., 644., 617.],\n",
      "        [638., 620., 566.,  ..., 644., 617., 614.]])\n",
      "tensor([[620., 566., 468.,  ..., 617., 614., 569.],\n",
      "        [566., 468., 478.,  ..., 614., 569., 655.],\n",
      "        [468., 478., 545.,  ..., 569., 655., 513.],\n",
      "        ...,\n",
      "        [560., 527., 647.,  ..., 449., 540., 532.],\n",
      "        [527., 647., 724.,  ..., 540., 532., 569.],\n",
      "        [647., 724., 488.,  ..., 532., 569., 510.]])\n",
      "tensor([[ 724.,  488.,  509.,  ...,  569.,  510.,  500.],\n",
      "        [ 488.,  509.,  540.,  ...,  510.,  500.,  515.],\n",
      "        [ 509.,  540.,  533.,  ...,  500.,  515.,  492.],\n",
      "        ...,\n",
      "        [ 426.,  495., 1577.,  ...,  591.,  631.,  570.],\n",
      "        [ 495., 1577.,  806.,  ...,  631.,  570.,  514.],\n",
      "        [1577.,  806.,  605.,  ...,  570.,  514.,  535.]])\n",
      "tensor([[ 806.,  605.,  486.,  ...,  514.,  535., 1711.],\n",
      "        [ 605.,  486.,  466.,  ...,  535., 1711.,  769.],\n",
      "        [ 486.,  466.,  529.,  ..., 1711.,  769.,  623.],\n",
      "        ...,\n",
      "        [ 763.,  755.,  727.,  ...,  941.,  855.,  739.],\n",
      "        [ 755.,  727.,  784.,  ...,  855.,  739.,  761.],\n",
      "        [ 727.,  784.,  884.,  ...,  739.,  761.,  645.]])\n",
      "tensor([[ 784.,  884.,  816.,  ...,  761.,  645.,  683.],\n",
      "        [ 884.,  816.,  732.,  ...,  645.,  683.,  690.],\n",
      "        [ 816.,  732.,  688.,  ...,  683.,  690.,  751.],\n",
      "        ...,\n",
      "        [1323., 1120.,  955.,  ...,  630., 1189.,  852.],\n",
      "        [1120.,  955.,  755.,  ..., 1189.,  852.,  874.],\n",
      "        [ 955.,  755.,  836.,  ...,  852.,  874., 1157.]])\n",
      "tensor([[ 755.,  836.,  719.,  ...,  874., 1157., 1054.],\n",
      "        [ 836.,  719.,  746.,  ..., 1157., 1054.,  771.],\n",
      "        [ 719.,  746.,  790.,  ..., 1054.,  771., 1198.],\n",
      "        ...,\n",
      "        [ 893.,  872.,  808.,  ...,  922., 1091.,  999.],\n",
      "        [ 872.,  808.,  843.,  ..., 1091.,  999.,  765.],\n",
      "        [ 808.,  843.,  923.,  ...,  999.,  765.,  924.]])\n",
      "tensor([[ 843.,  923., 1145.,  ...,  765.,  924.,  790.],\n",
      "        [ 923., 1145., 1145.,  ...,  924.,  790.,  884.],\n",
      "        [1145., 1145.,  876.,  ...,  790.,  884.,  914.],\n",
      "        ...,\n",
      "        [ 604.,  606.,  571.,  ...,  616.,  612.,  696.],\n",
      "        [ 606.,  571.,  609.,  ...,  612.,  696.,  681.],\n",
      "        [ 571.,  609.,  529.,  ...,  696.,  681.,  531.]])\n",
      "tensor([[609., 529., 612.,  ..., 681., 531., 649.],\n",
      "        [529., 612., 622.,  ..., 531., 649., 623.],\n",
      "        [612., 622., 626.,  ..., 649., 623., 713.],\n",
      "        ...,\n",
      "        [483., 428., 474.,  ..., 453., 473., 780.],\n",
      "        [428., 474., 568.,  ..., 473., 780., 464.],\n",
      "        [474., 568., 450.,  ..., 780., 464., 388.]])\n",
      "tensor([[568., 450., 468.,  ..., 464., 388., 356.],\n",
      "        [450., 468., 506.,  ..., 388., 356., 393.],\n",
      "        [468., 506., 457.,  ..., 356., 393., 419.],\n",
      "        ...,\n",
      "        [886., 379., 373.,  ..., 289., 329., 371.],\n",
      "        [379., 373., 391.,  ..., 329., 371., 359.],\n",
      "        [373., 391., 352.,  ..., 371., 359., 392.]])\n",
      "tensor([[391., 352., 395.,  ..., 359., 392., 356.],\n",
      "        [352., 395., 405.,  ..., 392., 356., 341.],\n",
      "        [395., 405., 363.,  ..., 356., 341., 336.],\n",
      "        ...,\n",
      "        [353., 270., 274.,  ..., 325., 315., 332.],\n",
      "        [353., 270., 274.,  ..., 325., 315., 332.],\n",
      "        [353., 270., 274.,  ..., 325., 315., 332.]])\n",
      "tensor([[  353.,   270.,   274.,  ...,   325.,   315.,   332.],\n",
      "        [  353.,   270.,   274.,  ...,   325.,   315.,   332.],\n",
      "        [  353.,   270.,   274.,  ...,   325.,   315.,   332.],\n",
      "        ...,\n",
      "        [10360., 12224., 12372.,  ...,  2859.,  2683.,  2259.],\n",
      "        [12224., 12372., 11853.,  ...,  2683.,  2259.,  3375.],\n",
      "        [12372., 11853., 13006.,  ...,  2259.,  3375.,  1729.]])\n",
      "tensor([[11853., 13006., 15267.,  ...,  3375.,  1729.,  1310.],\n",
      "        [13006., 15267., 21197.,  ...,  1729.,  1310.,  1044.],\n",
      "        [15267., 21197., 24176.,  ...,  1310.,  1044.,   980.],\n",
      "        ...,\n",
      "        [  488.,   486.,   533.,  ...,   734.,   821.,   844.],\n",
      "        [  486.,   533.,   551.,  ...,   821.,   844.,   982.],\n",
      "        [  533.,   551.,   712.,  ...,   844.,   982.,  1176.]])\n",
      "tensor([[ 551.,  712.,  654.,  ...,  982., 1176., 1450.],\n",
      "        [ 712.,  654.,  591.,  ..., 1176., 1450., 1772.],\n",
      "        [ 654.,  591.,  708.,  ..., 1450., 1772.,  889.],\n",
      "        ...,\n",
      "        [ 452.,  392.,  399.,  ...,  439.,  439.,  425.],\n",
      "        [ 392.,  399.,  486.,  ...,  439.,  425.,  453.],\n",
      "        [ 399.,  486.,  403.,  ...,  425.,  453.,  450.]])\n",
      "tensor([[ 486.,  403.,  378.,  ...,  453.,  450.,  492.],\n",
      "        [ 403.,  378.,  420.,  ...,  450.,  492.,  439.],\n",
      "        [ 378.,  420.,  357.,  ...,  492.,  439.,  416.],\n",
      "        ...,\n",
      "        [ 892., 1484., 1165.,  ..., 1026.,  962., 1013.],\n",
      "        [1484., 1165., 1049.,  ...,  962., 1013., 1046.],\n",
      "        [1165., 1049.,  973.,  ..., 1013., 1046., 1031.]])\n",
      "tensor([[1049.,  973.,  893.,  ..., 1046., 1031.,  982.],\n",
      "        [ 973.,  893.,  937.,  ..., 1031.,  982.,  967.],\n",
      "        [ 893.,  937.,  938.,  ...,  982.,  967.,  914.],\n",
      "        ...,\n",
      "        [ 802.,  746.,  881.,  ...,  803.,  904.,  831.],\n",
      "        [ 746.,  881.,  733.,  ...,  904.,  831.,  739.],\n",
      "        [ 881.,  733.,  823.,  ...,  831.,  739.,  766.]])\n",
      "tensor([[733., 823., 847.,  ..., 739., 766., 810.],\n",
      "        [823., 847., 853.,  ..., 766., 810., 807.],\n",
      "        [847., 853., 858.,  ..., 810., 807., 880.],\n",
      "        ...,\n",
      "        [666., 658., 532.,  ..., 538., 481., 563.],\n",
      "        [658., 532., 458.,  ..., 481., 563., 508.],\n",
      "        [532., 458., 526.,  ..., 563., 508., 532.]])\n",
      "tensor([[ 458.,  526.,  534.,  ...,  508.,  532.,  564.],\n",
      "        [ 526.,  534.,  491.,  ...,  532.,  564.,  514.],\n",
      "        [ 534.,  491.,  978.,  ...,  564.,  514.,  542.],\n",
      "        ...,\n",
      "        [1095., 1010.,  994.,  ..., 1008., 1105., 1071.],\n",
      "        [1010.,  994.,  929.,  ..., 1105., 1071., 1534.],\n",
      "        [ 994.,  929.,  940.,  ..., 1071., 1534., 1338.]])\n",
      "tensor([[ 929.,  940.,  956.,  ..., 1534., 1338., 1239.],\n",
      "        [ 940.,  956., 1062.,  ..., 1338., 1239., 1313.],\n",
      "        [ 956., 1062.,  916.,  ..., 1239., 1313., 1372.],\n",
      "        ...,\n",
      "        [ 941.,  912.,  915.,  ...,  924.,  912.,  959.],\n",
      "        [ 912.,  915., 1058.,  ...,  912.,  959., 1059.],\n",
      "        [ 915., 1058., 1124.,  ...,  959., 1059., 1040.]])\n",
      "tensor([[1058., 1124.,  982.,  ..., 1059., 1040., 1049.],\n",
      "        [1124.,  982., 1003.,  ..., 1040., 1049., 1147.],\n",
      "        [ 982., 1003.,  966.,  ..., 1049., 1147., 1141.],\n",
      "        ...,\n",
      "        [ 980., 1129., 1200.,  ..., 1130., 1027.,  968.],\n",
      "        [1129., 1200., 1102.,  ..., 1027.,  968.,  929.],\n",
      "        [1200., 1102., 1055.,  ...,  968.,  929.,  948.]])\n",
      "tensor([[1102., 1055.,  975.,  ...,  929.,  948., 1035.],\n",
      "        [1055.,  975.,  937.,  ...,  948., 1035., 1041.],\n",
      "        [ 975.,  937.,  946.,  ..., 1035., 1041., 1014.],\n",
      "        ...,\n",
      "        [2361., 2249., 2050.,  ..., 1032., 1037.,  908.],\n",
      "        [2249., 2050., 1621.,  ..., 1037.,  908., 1014.],\n",
      "        [2050., 1621., 1325.,  ...,  908., 1014.,  980.]])\n",
      "tensor([[1621., 1325., 1299.,  ..., 1014.,  980., 1390.],\n",
      "        [1325., 1299., 1045.,  ...,  980., 1390., 1647.],\n",
      "        [1299., 1045., 1136.,  ..., 1390., 1647., 1087.],\n",
      "        ...,\n",
      "        [ 558.,  575.,  636.,  ...,  571.,  894., 1257.],\n",
      "        [ 575.,  636.,  552.,  ...,  894., 1257., 1134.],\n",
      "        [ 636.,  552.,  579.,  ..., 1257., 1134.,  696.]])\n",
      "tensor([[ 552.,  579.,  611.,  ..., 1134.,  696.,  577.],\n",
      "        [ 579.,  611.,  557.,  ...,  696.,  577.,  564.],\n",
      "        [ 611.,  557.,  674.,  ...,  577.,  564.,  540.],\n",
      "        ...,\n",
      "        [ 700.,  585.,  540.,  ...,  539.,  562.,  563.],\n",
      "        [ 585.,  540.,  556.,  ...,  562.,  563.,  520.],\n",
      "        [ 540.,  556.,  658.,  ...,  563.,  520.,  566.]])\n",
      "tensor([[556., 658., 631.,  ..., 520., 566., 490.],\n",
      "        [658., 631., 613.,  ..., 566., 490., 496.],\n",
      "        [631., 613., 606.,  ..., 490., 496., 531.],\n",
      "        ...,\n",
      "        [732., 759., 767.,  ..., 635., 599., 521.],\n",
      "        [759., 767., 708.,  ..., 599., 521., 567.],\n",
      "        [767., 708., 699.,  ..., 521., 567., 613.]])\n",
      "tensor([[708., 699., 675.,  ..., 567., 613., 652.],\n",
      "        [699., 675., 772.,  ..., 613., 652., 593.],\n",
      "        [675., 772., 683.,  ..., 652., 593., 496.],\n",
      "        ...,\n",
      "        [ 89., 103., 123.,  ..., 108.,  83.,  86.],\n",
      "        [103., 123., 126.,  ...,  83.,  86.,  78.],\n",
      "        [123., 126.,  92.,  ...,  86.,  78.,  94.]])\n",
      "tensor([[126.,  92.,  83.,  ...,  78.,  94.,  92.],\n",
      "        [ 92.,  83.,  82.,  ...,  94.,  92., 119.],\n",
      "        [ 83.,  82.,  85.,  ...,  92., 119., 107.],\n",
      "        ...,\n",
      "        [161., 140., 140.,  ..., 106.,  84., 116.],\n",
      "        [140., 140., 101.,  ...,  84., 116., 131.],\n",
      "        [140., 101., 107.,  ..., 116., 131., 109.]])\n",
      "tensor([[101., 107.,  93.,  ..., 131., 109., 129.],\n",
      "        [107.,  93., 116.,  ..., 109., 129.,  83.],\n",
      "        [ 93., 116., 141.,  ..., 129.,  83.,  97.],\n",
      "        ...,\n",
      "        [125., 135., 135.,  ..., 528., 218., 171.],\n",
      "        [135., 135., 151.,  ..., 218., 171., 168.],\n",
      "        [135., 151., 231.,  ..., 171., 168., 148.]])\n",
      "tensor([[151., 231., 211.,  ..., 168., 148., 136.],\n",
      "        [231., 211., 163.,  ..., 148., 136., 151.],\n",
      "        [211., 163., 133.,  ..., 136., 151., 223.],\n",
      "        ...,\n",
      "        [145.,  99.,  91.,  ...,  99., 145., 145.],\n",
      "        [145.,  99.,  91.,  ...,  99., 145., 145.],\n",
      "        [145.,  99.,  91.,  ...,  99., 145., 145.]])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\MOHAMM~1\\AppData\\Local\\Temp/ipykernel_13012/975876602.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m     train=False, batch_size=batch_size * 10, num_workers=0)\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0miter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_dataloader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;31m# calculate baseline mean absolute error, i.e. predict next value as the last available value from the history\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    679\u001b[0m                 \u001b[1;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    680\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 681\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    682\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    683\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    719\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    720\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 721\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    722\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    723\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pytorch_forecasting\\data\\timeseries.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m   1575\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_relative_time_idx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1576\u001b[0m             data_cont[:, self.reals.index(\"relative_time_idx\")] = (\n\u001b[1;32m-> 1577\u001b[1;33m                 \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mencoder_length\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecoder_length\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata_cont\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_encoder_length\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1578\u001b[0m             )\n\u001b[0;32m   1579\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# create dataloaders for model\n",
    "batch_size = 64  # set this between 32 to 128\n",
    "train_dataloader = dataset.to_dataloader(\n",
    "    train=True, batch_size=batch_size, num_workers=0)\n",
    "val_dataloader = dataset.to_dataloader(\n",
    "    train=False, batch_size=batch_size * 10, num_workers=0)\n",
    "\n",
    "# calculate baseline mean absolute error, i.e. predict next value as the last available value from the history\n",
    "# actuals = torch.cat([y for x, (y, weight) in iter(val_dataloader)])\n",
    "# baseline_predictions = Baseline().predict(val_dataloader)\n",
    "# (actuals - baseline_predictions).abs().mean().item()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d7baccf88227eab4c3a997091e3b8cad5657bd0277ceacd9564be0b0f999d1db"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
