{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks import EarlyStopping, LearningRateMonitor\n",
    "from torch.utils.data import Dataset,DataLoader,TensorDataset\n",
    "from pytorch_forecasting import TimeSeriesDataSet\n",
    "from pytorch_forecasting.models import BaseModel\n",
    "from pytorch_forecasting.data import GroupNormalizer\n",
    "from pytorch_forecasting import Baseline\n",
    "import matplotlib.pyplot as plt\n",
    "from pytorch_lightning import Trainer\n",
    "from typing import Dict\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "#os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>views</th>\n",
       "      <th>month</th>\n",
       "      <th>log_views</th>\n",
       "      <th>time_idx</th>\n",
       "      <th>dayofweek</th>\n",
       "      <th>year</th>\n",
       "      <th>avg_by_song</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(Everything_I_Do)_I_Do_It_for_You</td>\n",
       "      <td>2015-07-01</td>\n",
       "      <td>639</td>\n",
       "      <td>7</td>\n",
       "      <td>6.459904</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2015</td>\n",
       "      <td>682.548387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(Everything_I_Do)_I_Do_It_for_You</td>\n",
       "      <td>2015-07-02</td>\n",
       "      <td>647</td>\n",
       "      <td>7</td>\n",
       "      <td>6.472346</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2015</td>\n",
       "      <td>682.548387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(Everything_I_Do)_I_Do_It_for_You</td>\n",
       "      <td>2015-07-03</td>\n",
       "      <td>549</td>\n",
       "      <td>7</td>\n",
       "      <td>6.308098</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2015</td>\n",
       "      <td>682.548387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(Everything_I_Do)_I_Do_It_for_You</td>\n",
       "      <td>2015-07-04</td>\n",
       "      <td>638</td>\n",
       "      <td>7</td>\n",
       "      <td>6.458338</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>2015</td>\n",
       "      <td>682.548387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(Everything_I_Do)_I_Do_It_for_You</td>\n",
       "      <td>2015-07-05</td>\n",
       "      <td>774</td>\n",
       "      <td>7</td>\n",
       "      <td>6.651572</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>2015</td>\n",
       "      <td>682.548387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150686</th>\n",
       "      <td>You're_the_One_That_I_Want</td>\n",
       "      <td>2021-12-28</td>\n",
       "      <td>217</td>\n",
       "      <td>12</td>\n",
       "      <td>5.379897</td>\n",
       "      <td>2372</td>\n",
       "      <td>1</td>\n",
       "      <td>2021</td>\n",
       "      <td>191.064516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150687</th>\n",
       "      <td>You're_the_One_That_I_Want</td>\n",
       "      <td>2021-12-29</td>\n",
       "      <td>181</td>\n",
       "      <td>12</td>\n",
       "      <td>5.198497</td>\n",
       "      <td>2373</td>\n",
       "      <td>2</td>\n",
       "      <td>2021</td>\n",
       "      <td>191.064516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150688</th>\n",
       "      <td>You're_the_One_That_I_Want</td>\n",
       "      <td>2021-12-30</td>\n",
       "      <td>175</td>\n",
       "      <td>12</td>\n",
       "      <td>5.164786</td>\n",
       "      <td>2374</td>\n",
       "      <td>3</td>\n",
       "      <td>2021</td>\n",
       "      <td>191.064516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150689</th>\n",
       "      <td>You're_the_One_That_I_Want</td>\n",
       "      <td>2021-12-31</td>\n",
       "      <td>279</td>\n",
       "      <td>12</td>\n",
       "      <td>5.631212</td>\n",
       "      <td>2375</td>\n",
       "      <td>4</td>\n",
       "      <td>2021</td>\n",
       "      <td>191.064516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150690</th>\n",
       "      <td>You're_the_One_That_I_Want</td>\n",
       "      <td>2022-01-01</td>\n",
       "      <td>221</td>\n",
       "      <td>1</td>\n",
       "      <td>5.398163</td>\n",
       "      <td>2376</td>\n",
       "      <td>5</td>\n",
       "      <td>2022</td>\n",
       "      <td>221.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>121227 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  article  timestamp  views month  log_views  \\\n",
       "0       (Everything_I_Do)_I_Do_It_for_You 2015-07-01    639     7   6.459904   \n",
       "1       (Everything_I_Do)_I_Do_It_for_You 2015-07-02    647     7   6.472346   \n",
       "2       (Everything_I_Do)_I_Do_It_for_You 2015-07-03    549     7   6.308098   \n",
       "3       (Everything_I_Do)_I_Do_It_for_You 2015-07-04    638     7   6.458338   \n",
       "4       (Everything_I_Do)_I_Do_It_for_You 2015-07-05    774     7   6.651572   \n",
       "...                                   ...        ...    ...   ...        ...   \n",
       "150686         You're_the_One_That_I_Want 2021-12-28    217    12   5.379897   \n",
       "150687         You're_the_One_That_I_Want 2021-12-29    181    12   5.198497   \n",
       "150688         You're_the_One_That_I_Want 2021-12-30    175    12   5.164786   \n",
       "150689         You're_the_One_That_I_Want 2021-12-31    279    12   5.631212   \n",
       "150690         You're_the_One_That_I_Want 2022-01-01    221     1   5.398163   \n",
       "\n",
       "        time_idx dayofweek  year  avg_by_song  \n",
       "0              0         2  2015   682.548387  \n",
       "1              1         3  2015   682.548387  \n",
       "2              2         4  2015   682.548387  \n",
       "3              3         5  2015   682.548387  \n",
       "4              4         6  2015   682.548387  \n",
       "...          ...       ...   ...          ...  \n",
       "150686      2372         1  2021   191.064516  \n",
       "150687      2373         2  2021   191.064516  \n",
       "150688      2374         3  2021   191.064516  \n",
       "150689      2375         4  2021   191.064516  \n",
       "150690      2376         5  2022   221.000000  \n",
       "\n",
       "[121227 rows x 9 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"data/All_music.csv\")\n",
    "data.sort_values(by=[\"timestamp\"],inplace=True) # for my sanity\n",
    "data[\"timestamp\"] = pd.to_datetime(data[\"timestamp\"]) # needed to create time_index\n",
    "\n",
    "#group by month\n",
    "# data[\"time_idx\"] = data[\"timestamp\"].dt.year * 12 + data[\"timestamp\"].dt.month\n",
    "# data[\"time_idx\"] -= data[\"time_idx\"].min()\n",
    "\n",
    "data[\"month\"] = data[\"timestamp\"].dt.month.astype(str).astype(\"category\")\n",
    "data[\"log_views\"] = np.log(data[\"views\"] + 1e-8)\n",
    "\n",
    "\n",
    "data[\"time_idx\"] = data.index.astype(int)\n",
    "#data[\"timestamp\"] = data[\"timestamp\"].astype(str).astype('category')\n",
    "# (data.groupby(\"project\")[\"time_idx\"].diff() == 1).value_counts()\n",
    "\n",
    "\n",
    "# # add time index\n",
    "# data[\"time_idx\"] = data[\"date\"].dt.year * 12 + data[\"date\"].dt.month\n",
    "# data[\"time_idx\"] -= data[\"time_idx\"].min()\n",
    "data.drop([\"project\",\"granularity\",\"access\",\"agent\"],inplace=True,axis=1)\n",
    "data[\"article\"] = data[\"article\"].astype(str).astype(\"category\")\n",
    "\n",
    "# Delete wrong data\n",
    "data = data[data[\"article\"] != \"Paper_Doll\"]\n",
    "data = data[data[\"article\"] != \"Y.M.C.A\"]\n",
    "data = data[data[\"article\"] != \"Bei_Mir_Bist_Du_Schön\"]\n",
    "\n",
    "#new Features\n",
    "data[\"dayofweek\"] = data[\"timestamp\"].dt.dayofweek.astype(str).astype(\"category\")\n",
    "data[\"log_views\"] = np.log(data[\"views\"] + 1e-8)\n",
    "data[\"month\"] = data[\"timestamp\"].dt.month.astype(str).astype(\"category\")\n",
    "data[\"year\"] = data[\"timestamp\"].dt.year.astype(str).astype(\"category\")\n",
    "data[\"avg_by_song\"] = data.groupby([\"month\",\"year\",\"article\"],observed=True)[\"views\"].transform(\"mean\")\n",
    "\n",
    "\n",
    "data = data[data['article'].map(data['article'].value_counts()) == 2377]\n",
    "data.drop(\"Unnamed: 0\",inplace=True,axis=1)\n",
    "data.sort_values(by=[\"article\",\"timestamp\"],inplace=True)\n",
    "#data[\"time_idx\"] = data[\"timestamp\"].dt.year * 12 + data[\"timestamp\"].dt.month + 365 * data[\"timestamp\"].dt.day\n",
    "data[\"time_idx\"] = [i for i in range(2377)] * len(data[\"article\"].unique())\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mohammad Sakhnini\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:268: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Mohammad Sakhnini\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:268: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Mohammad Sakhnini\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch\\nn\\modules\\rnn.py:62: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "C:\\Users\\Mohammad Sakhnini\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:447: LightningDeprecationWarning: Setting `Trainer(gpus=1)` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=1)` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..\n"
     ]
    }
   ],
   "source": [
    "from torch.nn.utils import rnn\n",
    "\n",
    "from pytorch_forecasting.models.base_model import AutoRegressiveBaseModel\n",
    "from pytorch_forecasting.models.nn import LSTM\n",
    "\n",
    "\n",
    "class LSTMModel(AutoRegressiveBaseModel):\n",
    "    def __init__(\n",
    "        self,\n",
    "        target: str,\n",
    "        target_lags: Dict[str, Dict[str, int]],\n",
    "        n_layers: int,\n",
    "        hidden_size: int,\n",
    "        dropout: float = 0.1,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        # arguments target and target_lags are required for autoregressive models\n",
    "        # even though target_lags cannot be used without covariates\n",
    "        # saves arguments in signature to `.hparams` attribute, mandatory call - do not skip this\n",
    "        self.save_hyperparameters()\n",
    "        # pass additional arguments to BaseModel.__init__, mandatory call - do not skip this\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        # use version of LSTM that can handle zero-length sequences\n",
    "        self.lstm = LSTM(\n",
    "            hidden_size=self.hparams.hidden_size,\n",
    "            input_size=7,\n",
    "            num_layers=self.hparams.n_layers,\n",
    "            dropout=self.hparams.dropout,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.output_layer = nn.Linear(self.hparams.hidden_size, 1)\n",
    "\n",
    "    def encode(self, x: Dict[str, torch.Tensor]):\n",
    "        # we need at least one encoding step as because the target needs to be lagged by one time step\n",
    "        # because we use the custom LSTM, we do not have to require encoder lengths of > 1\n",
    "        # but can handle lengths of >= 1\n",
    "        assert x[\"encoder_lengths\"].min() >= 1\n",
    "        input_vector = x[\"encoder_cont\"].clone()\n",
    "        # lag target by one\n",
    "        input_vector[..., self.target_positions] = torch.roll(\n",
    "            input_vector[..., self.target_positions], shifts=1, dims=1\n",
    "        )\n",
    "        input_vector = input_vector[:, 1:]  # first time step cannot be used because of lagging\n",
    "\n",
    "        # determine effective encoder_length length\n",
    "        effective_encoder_lengths = x[\"encoder_lengths\"] - 1\n",
    "        # run through LSTM network\n",
    "        _, hidden_state = self.lstm(\n",
    "            input_vector, lengths=effective_encoder_lengths, enforce_sorted=False  # passing the lengths directly\n",
    "        )  # second ouput is not needed (hidden state)\n",
    "        return hidden_state\n",
    "\n",
    "    def decode(self, x: Dict[str, torch.Tensor], hidden_state):\n",
    "        # again lag target by one\n",
    "        input_vector = x[\"decoder_cont\"].clone()\n",
    "        input_vector[..., self.target_positions] = torch.roll(\n",
    "            input_vector[..., self.target_positions], shifts=1, dims=1\n",
    "        )\n",
    "        # but this time fill in missing target from encoder_cont at the first time step instead of throwing it away\n",
    "        last_encoder_target = x[\"encoder_cont\"][\n",
    "            torch.arange(x[\"encoder_cont\"].size(0), device=x[\"encoder_cont\"].device),\n",
    "            x[\"encoder_lengths\"] - 1,\n",
    "            self.target_positions.unsqueeze(-1),\n",
    "        ].T\n",
    "        input_vector[:, 0, self.target_positions] = last_encoder_target\n",
    "\n",
    "        if self.training:  # training mode\n",
    "            lstm_output, _ = self.lstm(input_vector, hidden_state, lengths=x[\"decoder_lengths\"], enforce_sorted=False)\n",
    "\n",
    "            # transform into right shape\n",
    "            prediction = self.output_layer(lstm_output)\n",
    "            prediction = self.transform_output(prediction, target_scale=x[\"target_scale\"])\n",
    "\n",
    "            # predictions are not yet rescaled\n",
    "            return prediction\n",
    "\n",
    "        else:  # prediction mode\n",
    "            target_pos = self.target_positions\n",
    "\n",
    "            def decode_one(idx, lagged_targets, hidden_state):\n",
    "                x = input_vector[:, [idx]]\n",
    "                # overwrite at target positions\n",
    "                x[:, 0, target_pos] = lagged_targets[-1]  # take most recent target (i.e. lag=1)\n",
    "                lstm_output, hidden_state = self.lstm(x, hidden_state)\n",
    "                # transform into right shape\n",
    "                prediction = self.output_layer(lstm_output)[:, 0]  # take first timestep\n",
    "                return prediction, hidden_state\n",
    "\n",
    "            # make predictions which are fed into next step\n",
    "            output = self.decode_autoregressive(\n",
    "                decode_one,\n",
    "                first_target=input_vector[:, 0, target_pos],\n",
    "                first_hidden_state=hidden_state,\n",
    "                target_scale=x[\"target_scale\"],\n",
    "                n_decoder_steps=input_vector.size(1),\n",
    "            )\n",
    "\n",
    "            # predictions are already rescaled\n",
    "            return output\n",
    "\n",
    "    def forward(self, x: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:\n",
    "        hidden_state = self.encode(x)  # encode to hidden state\n",
    "        output = self.decode(x, hidden_state)  # decode leveraging hidden state\n",
    "\n",
    "        return self.to_network_output(prediction=output)\n",
    "\n",
    "\n",
    "max_encoder_length = 365   #\n",
    "max_prediction_length = 14  # predict last 14 days\n",
    "test_cutoff = data.time_idx.max() - max_prediction_length\n",
    "training_cutoff = test_cutoff - max_prediction_length\n",
    "\n",
    "HIDDEN_SIZE = 16\n",
    "HIDDEN_CONTINOUS_SIZE = HIDDEN_SIZE // 2\n",
    "#create test set.\n",
    "data_test = data[lambda x:  test_cutoff < x.time_idx]\n",
    "\n",
    "# create train+val part\n",
    "data_seen = data[lambda x:  x.time_idx <= test_cutoff]\n",
    "\n",
    "\n",
    "dataset = TimeSeriesDataSet(\n",
    "    data[lambda x: x.time_idx <= training_cutoff],\n",
    "    time_idx=\"time_idx\",\n",
    "    target=\"views\",\n",
    "    group_ids=[\"article\"],\n",
    "    min_encoder_length=max_encoder_length // 2,\n",
    "    max_encoder_length=max_encoder_length,\n",
    "    min_prediction_length=1,\n",
    "    max_prediction_length=max_prediction_length,\n",
    "    target_normalizer=GroupNormalizer(groups=[\"article\"]),\n",
    "    time_varying_known_categoricals=[\"dayofweek\", \"month\", \"year\"],\n",
    "    time_varying_known_reals=[\"log_views\"],\n",
    "    static_categoricals=[\"article\"],\n",
    "    time_varying_unknown_reals=[\"avg_by_song\", \"log_views\"],\n",
    "    #allow_missing_timesteps=True,\n",
    "    add_relative_time_idx=True,\n",
    "    add_target_scales=True,\n",
    "    add_encoder_length=True)\n",
    "\n",
    "model = LSTMModel.from_dataset(dataset, n_layers=1, hidden_size=4)\n",
    "dataloader = dataset.to_dataloader(batch_size=4,shuffle=False)\n",
    "\n",
    "early_stop_callback = EarlyStopping(monitor=\"val_loss\", min_delta=1e-4, patience=10, verbose=False, mode=\"min\")\n",
    "lr_logger = LearningRateMonitor()  # log the learning rate\n",
    "\n",
    "trainer = Trainer(\n",
    "    max_epochs=20,\n",
    "    gpus=1,\n",
    "    gradient_clip_val=0.1,\n",
    "    #limit_train_batches=30,  # coment in for training, running valiation every 30 batches\n",
    "    limit_train_batches=1.0, #if set to 1.0 gather all training data, default.\n",
    "    callbacks=[lr_logger, early_stop_callback],\n",
    ")\n",
    "path = \"model_lstm.pth\"\n",
    "\n",
    "if os.path.exists(path):\n",
    "    model.load_state_dict(torch.load(path))\n",
    "    model.eval()\n",
    "else:\n",
    "    trainer.fit(\n",
    "    model,\n",
    "    train_dataloaders=dataloader,\n",
    "    val_dataloaders=dataloader,\n",
    ")\n",
    "#trainer.fit(model, train_dataloaders=dataloader, val_dataloaders=dataloader)\n",
    "#torch.save(model.state_dict(),path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataloaders for model\n",
    "batch_size = 64  # set this between 32 to 128\n",
    "train_dataloader = dataset.to_dataloader(\n",
    "    train=True, batch_size=batch_size, num_workers=0)\n",
    "val_dataloader = dataset.to_dataloader(\n",
    "    train=False, batch_size=batch_size * 10, num_workers=0)\n",
    "\n",
    "# calculate baseline mean absolute error, i.e. predict next value as the last available value from the history\n",
    "# actuals = torch.cat([y for x, (y, weight) in iter(val_dataloader)])\n",
    "# baseline_predictions = Baseline().predict(val_dataloader)\n",
    "# (actuals - baseline_predictions).abs().mean().item()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 (tags/v3.9.13:6de2ca5, May 17 2022, 16:36:42) [MSC v.1929 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d7baccf88227eab4c3a997091e3b8cad5657bd0277ceacd9564be0b0f999d1db"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
